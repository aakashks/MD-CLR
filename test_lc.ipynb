{"cells":[{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["#/r"]},{"cell_type":"markdown","metadata":{"id":"JC-p4CpT1_Lc"},"source":["# Load Data"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T05:12:51.017424Z","iopub.status.busy":"2024-04-16T05:12:51.016653Z","iopub.status.idle":"2024-04-16T05:12:51.028310Z","shell.execute_reply":"2024-04-16T05:12:51.027476Z","shell.execute_reply.started":"2024-04-16T05:12:51.017395Z"},"id":"gr9MjCphpOPx","trusted":true},"outputs":[],"source":["OUTPUT_FOLDER = \"/scratch/aakash_ks.iitr/dr-scnn/\"\n","DATA_FOLDER = \"/scratch/aakash_ks.iitr/data/diabetic-retinopathy/\"\n","# TRAIN_DATA_FOLDER = DATA_FOLDER + 'resized_train/'\n","TRAIN_DATA_FOLDER = DATA_FOLDER + 'resized_train_c/'\n","\n","TEST_DATA_FOLDER = DATA_FOLDER + 'test/'"]},{"cell_type":"markdown","metadata":{"id":"dOaKi5h92DBb"},"source":["# Imports"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:13:45.703815Z","iopub.status.busy":"2024-04-14T16:13:45.703112Z","iopub.status.idle":"2024-04-14T16:13:47.998744Z","shell.execute_reply":"2024-04-14T16:13:47.997755Z","shell.execute_reply.started":"2024-04-14T16:13:45.703785Z"},"id":"NNdj2cxdkpiv","trusted":true},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","from PIL import Image\n","\n","plt.rcParams['figure.dpi'] = 100"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:13:48.000701Z","iopub.status.busy":"2024-04-14T16:13:48.000307Z","iopub.status.idle":"2024-04-14T16:13:55.723296Z","shell.execute_reply":"2024-04-14T16:13:55.722301Z","shell.execute_reply.started":"2024-04-14T16:13:48.000675Z"},"id":"7yoCqGCB2jIS","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n","from torchvision.transforms import v2\n","\n","import timm"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T05:18:58.885632Z","iopub.status.busy":"2024-04-16T05:18:58.884804Z","iopub.status.idle":"2024-04-16T05:18:58.891553Z","shell.execute_reply":"2024-04-16T05:18:58.890585Z","shell.execute_reply.started":"2024-04-16T05:18:58.885598Z"},"id":"8Ejzj4rDx_GK","trusted":true},"outputs":[],"source":["\n","NUM_CLASSES = 5\n","\n","class CFG:\n","    seed = 42\n","    N_folds = 6\n","    train_folds = [0, ] # [0,1,2,3,4]\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    apex=True # use half precision\n","    workers = 16\n","\n","    model_name = \"resnet50.a1_in1k\"\n","    epochs = 20\n","    cropped = True\n","    # weights =  torch.tensor([0.206119, 0.793881],dtype=torch.float32)\n","\n","    clip_val = 1000.\n","    batch_size = 64\n","    # gradient_accumulation_steps = 1\n","\n","    lr = 5e-3\n","    weight_decay=1e-2\n","    \n","    resolution = 224\n","    samples_per_class = 1000\n","    frozen_layers = 0"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T15:23:54.468659Z","iopub.status.busy":"2024-04-14T15:23:54.468265Z","iopub.status.idle":"2024-04-14T15:24:25.601196Z","shell.execute_reply":"2024-04-14T15:24:25.600218Z","shell.execute_reply.started":"2024-04-14T15:23:54.468630Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maakashks_\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/scratch/aakash_ks.iitr/dr-scnn/wandb/run-20240515_122219-gtp9i770</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/aakashks_/hello-world/runs/gtp9i770/workspace' target=\"_blank\">lilac-wind-118</a></strong> to <a href='https://wandb.ai/aakashks_/hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/aakashks_/hello-world' target=\"_blank\">https://wandb.ai/aakashks_/hello-world</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/aakashks_/hello-world/runs/gtp9i770/workspace' target=\"_blank\">https://wandb.ai/aakashks_/hello-world/runs/gtp9i770/workspace</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import wandb\n","# from kaggle_secrets import UserSecretsClient\n","# user_secrets = UserSecretsClient()\n","# wandb.login(key=user_secrets.get_secret(\"wandb_api\"))\n","\n","run = wandb.init(\n","    project=\"hello-world\", \n","    dir=OUTPUT_FOLDER,\n","    config={\n","    k:v for k, v in CFG.__dict__.items() if not k.startswith('__')}\n",")"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["from sklearn.manifold import TSNE\n","import matplotlib.colors as mcolors\n","\n","class LinearClassifier(nn.Module):\n","    def __init__(self, in_features=2048, num_classes=NUM_CLASSES):\n","        super().__init__()\n","        self.model = nn.Linear(in_features, num_classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","    \n","\n","class SupConModel(nn.Module):\n","    def __init__(self, encoder, input_dim=2048, output_dim=128):        # assuming either resnet50 or resnet101 is used\n","        super().__init__()\n","        self.encoder = encoder\n","        self.head = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, output_dim)\n","        )\n","    \n","    def forward(self, x):\n","        ft = self.encoder(x)\n","        return F.normalize(self.head(ft), dim=1)\n","\n","\n","class ImageTrainDataset(Dataset):\n","    def __init__(\n","            self,\n","            folder,\n","            data,\n","            transforms,\n","    ):\n","        self.folder = folder\n","        self.data = data\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        d = self.data.loc[index]\n","        image = Image.open(f\"{self.folder}{d.image}.jpeg\")\n","        image = self.transforms(image)\n","        label = d.level\n","\n","        return image, torch.tensor(label, dtype=torch.long)\n","\n","\n","def plot_tsne(embeddings, labels):\n","    # Apply t-SNE to the embeddings\n","    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n","    tsne_results = tsne.fit_transform(embeddings.numpy())\n","\n","    # Define the number of unique labels/classes\n","    num_classes = len(np.unique(labels.numpy()))\n","    # Create a custom color map with specific color transitions\n","    colors = ['blue', 'green', 'yellow', 'orange', 'red']\n","    cmap = mcolors.LinearSegmentedColormap.from_list(\"Custom\", colors, N=num_classes)\n","\n","    # Create a boundary norm with boundaries and colors\n","    norm = mcolors.BoundaryNorm(np.arange(-0.5, num_classes + 0.5, 1), cmap.N)\n","\n","    fig = plt.figure(figsize=(10, 8))\n","    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap=cmap, norm=norm, alpha=0.5)\n","    colorbar = plt.colorbar(scatter, ticks=np.arange(num_classes))\n","    colorbar.set_label('Severity Level')\n","    colorbar.set_ticklabels(np.arange(num_classes))  # Set discrete labels if needed\n","    plt.title('t-SNE of Image Embeddings with Discrete Severity Levels')\n","    plt.xlabel('t-SNE Axis 1')\n","    plt.ylabel('t-SNE Axis 2')\n","    fg = wandb.Image(fig)\n","    wandb.log({\"t-SNE\": fg})\n","    plt.savefig(os.path.join(wandb.run.dir, f\"tsne.png\"), dpi=300, bbox_inches='tight')\n","\n","\n","\n","class style:\n","    BLUE = '\\033[94m'\n","    GREEN = '\\033[92m'\n","    YELLOW = '\\033[93m'\n","    END = '\\033[0m'\n","    BOLD = '\\033[1m'\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["device = torch.device(CFG.device)"]},{"cell_type":"markdown","metadata":{"id":"1Mu24W3Xkpix"},"source":["# Dataset"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["from torchvision.transforms import functional as func\n","\n","class CustomTransform:\n","    def __init__(self, output_size=(CFG.resolution, CFG.resolution), radius_factor=0.9):\n","        self.output_size = output_size\n","        self.radius_factor = radius_factor\n","\n","    def __call__(self, img):\n","        # Assuming img is a PIL Image\n","        # Normalize and preprocess as previously defined\n","        img = func.resize(img, int(min(img.size) / self.radius_factor))\n","        img_tensor = func.to_tensor(img)\n","        mean, std = img_tensor.mean([1, 2]), img_tensor.std([1, 2])\n","        img_normalized = func.normalize(img_tensor, mean.tolist(), std.tolist())\n","        kernel_size = 15\n","        padding = kernel_size // 2\n","        avg_pool = torch.nn.AvgPool2d(kernel_size, stride=1, padding=padding)\n","        local_avg = avg_pool(img_normalized.unsqueeze(0)).squeeze(0)\n","        img_subtracted = img_normalized - local_avg\n","        center_crop_size = int(min(img_subtracted.shape[1:]) * self.radius_factor)\n","        img_cropped = func.center_crop(img_subtracted, [center_crop_size, center_crop_size])\n","\n","        # Apply augmentations\n","        img_resized = func.resize(img_cropped, self.output_size)\n","\n","        return img_resized"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:14:03.100814Z","iopub.status.busy":"2024-04-14T16:14:03.100062Z","iopub.status.idle":"2024-04-14T16:14:03.108402Z","shell.execute_reply":"2024-04-14T16:14:03.107471Z","shell.execute_reply.started":"2024-04-14T16:14:03.100781Z"},"trusted":true},"outputs":[],"source":["# train_transforms = CustomTransform()\n","\n","train_transforms = v2.Compose([\n","    v2.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2)),  # Gaussian blur with random kernel size and sigma\n","    v2.RandomRotation(degrees=(0, 90)),  # Random rotation between 0 and 360 degrees\n","    CustomTransform(),\n","    # v2.RandomResizedCrop(CFG.resolution, scale=(0.8, 1.0)),  # Krizhevsky style random cropping\n","    v2.RandomHorizontalFlip(),  # Random horizontal flip\n","    v2.RandomVerticalFlip(),  # Random vertical flip\n","    v2.ToDtype(torch.float32, scale=False),\n","])\n","\n","val_transforms = v2.Compose([\n","    CustomTransform(),\n","    v2.ToDtype(torch.float32, scale=False),\n","])"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/plain":["['DR2', 'Normal', 'DR3', 'DR1']"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir(TEST_DATA_FOLDER )"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from torchvision.datasets import ImageFolder\n"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["from torch.utils.data import random_split\n","\n","# visualize the transformations\n","data = ImageFolder(TEST_DATA_FOLDER, transform=val_transforms)\n","\n","# Define the ratio for the split\n","train_ratio = 0.75  # 75% training, 25% validation\n","train_size = int(train_ratio * len(data))\n","val_size = len(data) - train_size\n","\n","# Split the dataset\n","train_dataset, val_dataset = random_split(data, [train_size, val_size])"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"data":{"text/plain":["29"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["len(val_dataset)"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/plain":["{'DR1': 0, 'DR2': 1, 'DR3': 2, 'Normal': 3}"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["# split the data into train and validation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"OzgB1JpAv3qg"},"source":["# Metric"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:35.200919Z","iopub.status.busy":"2024-04-14T16:21:35.200562Z","iopub.status.idle":"2024-04-14T16:21:35.205583Z","shell.execute_reply":"2024-04-14T16:21:35.204624Z","shell.execute_reply.started":"2024-04-14T16:21:35.200892Z"},"id":"WNxSAhBrxJ-G","trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score as sklearn_f1\n","from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, precision_score"]},{"cell_type":"markdown","metadata":{"id":"Zyfw9PLdkpiz"},"source":["# Train and evaluate functions"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:36.353134Z","iopub.status.busy":"2024-04-14T16:21:36.352777Z","iopub.status.idle":"2024-04-14T16:21:36.363087Z","shell.execute_reply":"2024-04-14T16:21:36.362142Z","shell.execute_reply.started":"2024-04-14T16:21:36.353107Z"},"id":"yXcFJ6IYkpiz","trusted":true},"outputs":[],"source":["def evaluate_model(cfg, feature_extractor, model, data_loader, epoch=-1):\n","    targets = []\n","    predictions = []\n","\n","    total_len = len(data_loader)\n","    tk0 = tqdm(enumerate(data_loader), total=total_len)\n","    \n","    with torch.no_grad():\n","        for step, (images, labels) in tk0:\n","            images = images.to(device)\n","            target = labels.to(device)\n","            \n","            features = feature_extractor(images)\n","            logits = model(features)\n","            \n","            targets.append(target.detach().cpu())\n","            predictions.append(logits.detach().cpu())\n","            del images, target, logits\n","\n","    targets = torch.cat(targets, dim=0)\n","    predictions = torch.cat(predictions, dim=0)\n","    probabilities = F.softmax(predictions, dim=1)\n","\n","    # base_score, best_score, best_th = find_best_threshold(targets, predictions[:, 1])\n","    # For multi-class classification, you might need the class with the highest probability\n","    predicted_classes = predictions.argmax(dim=1)\n","\n","    try:\n","        wandb.log({\"roc\": wandb.plot.roc_curve(targets.numpy(), probabilities.numpy())})\n","        roc_auc = roc_auc_score(targets.numpy(), probabilities.numpy(), multi_class='ovo')\n","        \n","        wandb.log({\"pr\": wandb.plot.pr_curve(targets.numpy(), probabilities.numpy())})\n","        \n","    except:\n","        roc_auc = 0\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(targets.numpy(), predicted_classes.numpy())\n","\n","    precision = precision_score(targets.numpy(), predicted_classes.numpy(), average='weighted')\n","\n","    print(f'Epoch {epoch}: auc = {roc_auc:.4f} accuracy = {accuracy:.4f} precision = {precision:.4f}')\n","    return roc_auc, accuracy, precision"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:40.091094Z","iopub.status.busy":"2024-04-14T16:21:40.090402Z","iopub.status.idle":"2024-04-14T16:21:40.096027Z","shell.execute_reply":"2024-04-14T16:21:40.095120Z","shell.execute_reply.started":"2024-04-14T16:21:40.091064Z"},"trusted":true},"outputs":[],"source":["def create_model():\n","    # get the feature extractor\n","    resnet = timm.create_model(CFG.model_name, num_classes=0, pretrained=False)\n","    feature_extractor = SupConModel(resnet)\n","    feature_extractor.load_state_dict(torch.load(OUTPUT_FOLDER + 'ckpt_epoch_8.pth'))\n","    \n","    # remove the projection head\n","    feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])\n","\n","    # create a simple linear classifier\n","    classifier = LinearClassifier()\n","    classifier.load_state_dict(torch.load(OUTPUT_FOLDER + 'lc_sclr_11.pth'))\n","    return feature_extractor.to(device), classifier.to(device)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["from sklearn.manifold import TSNE\n","import matplotlib.colors as mcolors\n","\n","def get_embeddings(model, data_loader):\n","    model.eval()\n","    \n","    features = []\n","    targets = []\n","\n","    total_len = len(data_loader)\n","    tk0 = tqdm(enumerate(data_loader), total=total_len)\n","    with torch.no_grad():\n","        for step, (images, labels) in tk0:\n","            images = images.to(device)\n","            target = labels.to(device)\n","\n","            embds = model(images)\n","\n","            features.append(embds.detach().cpu())\n","            targets.append(target.detach().cpu())\n","\n","    features = torch.cat(features, dim=0)\n","    targets = torch.cat(targets, dim=0)\n","    \n","    # # store the embeddings for future use\n","    # torch.save(features, os.path.join(wandb.run.dir, f\"embeddings.pth\"))\n","    # torch.save(targets, os.path.join(wandb.run.dir, f\"targets.pth\"))\n","\n","    return features, targets\n"]},{"cell_type":"markdown","metadata":{"id":"rF9BFqS8AXBY"},"source":["## Train folds"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:40.983908Z","iopub.status.busy":"2024-04-14T16:21:40.983523Z","iopub.status.idle":"2024-04-14T16:21:48.524465Z","shell.execute_reply":"2024-04-14T16:21:48.523002Z","shell.execute_reply.started":"2024-04-14T16:21:40.983878Z"},"id":"7CFfmp3CxDG5","outputId":"952103d3-bbd9-449f-e9cf-26d5c2608aea","scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Layer conv1 is trainable.\n","Layer bn1 is trainable.\n","Layer act1 is trainable.\n","Layer maxpool is trainable.\n","Layer layer1 is trainable.\n","Layer layer2 is trainable.\n","Layer layer3 is trainable.\n","Layer layer4 is trainable.\n","Layer global_pool is trainable.\n","Layer fc is trainable.\n","Model parameters: 23_518_277\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 0 training 20/20 [LR 0.004969] - loss: 1.6485: 100%|███████████████| 20/20 [00:21<00:00,  1.09s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0: training loss = 1.6485 auc = 0.4868 accuracy = 0.2150 precision = 0.2188\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.90s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0: validation loss = 1.6251 auc = 0.0000 accuracy = 0.2143 precision = 0.0940\n","\u001b[92mNew best score: 0.0000 -> 0.2143\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["/home/aakash_ks.iitr/miniconda3/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","Epoch 1 training 20/20 [LR 0.004878] - loss: 1.5778: 100%|███████████████| 20/20 [00:21<00:00,  1.06s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: training loss = 1.5778 auc = 0.5908 accuracy = 0.2550 precision = 0.2785\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.93s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: validation loss = 1.6735 auc = 0.0000 accuracy = 0.2619 precision = 0.3811\n","\u001b[92mNew best score: 0.2143 -> 0.2619\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 training 20/20 [LR 0.004728] - loss: 1.5015: 100%|███████████████| 20/20 [00:21<00:00,  1.05s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: training loss = 1.5015 auc = 0.6787 accuracy = 0.3000 precision = 0.2893\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: validation loss = 2.1124 auc = 0.0000 accuracy = 0.2619 precision = 0.2526\n"]},{"name":"stderr","output_type":"stream","text":["/home/aakash_ks.iitr/miniconda3/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","Epoch 3 training 20/20 [LR 0.004523] - loss: 1.3510: 100%|███████████████| 20/20 [00:20<00:00,  1.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: training loss = 1.3510 auc = 0.7412 accuracy = 0.3800 precision = 0.3749\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.94s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: validation loss = 2.7870 auc = 0.0000 accuracy = 0.2143 precision = 0.1909\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 training 20/20 [LR 0.004268] - loss: 1.3476: 100%|███████████████| 20/20 [00:21<00:00,  1.06s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: training loss = 1.3476 auc = 0.7447 accuracy = 0.3550 precision = 0.3476\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.85s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: validation loss = 3.1248 auc = 0.0000 accuracy = 0.2381 precision = 0.1518\n"]},{"name":"stderr","output_type":"stream","text":["/home/aakash_ks.iitr/miniconda3/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","Epoch 5 training 20/20 [LR 0.003970] - loss: 1.3567: 100%|███████████████| 20/20 [00:20<00:00,  1.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: training loss = 1.3567 auc = 0.7494 accuracy = 0.3750 precision = 0.3971\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: validation loss = 2.2044 auc = 0.0000 accuracy = 0.2143 precision = 0.1821\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6 training 20/20 [LR 0.003635] - loss: 1.4598: 100%|███████████████| 20/20 [00:20<00:00,  1.04s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6: training loss = 1.4598 auc = 0.6979 accuracy = 0.3350 precision = 0.3336\n"]},{"name":"stderr","output_type":"stream","text":["  0%|                                                                              | 0/5 [00:05<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[71], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, CFG\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m     44\u001b[0m     train_loss, train_lr, train_auc, train_accuracy, train_precision \u001b[38;5;241m=\u001b[39m train_epoch(CFG, model, train_loader, loss_criterion, optimizer, scheduler, epoch)\n\u001b[0;32m---> 46\u001b[0m     val_loss, val_auc, val_accuracy, val_precision \u001b[38;5;241m=\u001b[39m evaluate_model(CFG, model, valid_loader, loss_criterion, epoch)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Log metrics to wandb\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss,\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_auc\u001b[39m\u001b[38;5;124m'\u001b[39m: train_auc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: train_lr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Log the last learning rate of the epoch\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     })\n","Cell \u001b[0;32mIn[65], line 15\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(cfg, model, data_loader, loss_criterion, epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m tk0 \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(data_loader), total\u001b[38;5;241m=\u001b[39mtotal_len)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, (images, labels) \u001b[38;5;129;01min\u001b[39;00m tk0:\n\u001b[1;32m     16\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m         target \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n","File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["seed_everything(CFG.seed)\n","\n","test_loader = DataLoader(\n","    test_data,\n","    batch_size=CFG.batch_size,\n","    shuffle=False,\n","    num_workers=CFG.workers,\n","    pin_memory=True,\n","    drop_last=False,\n",")\n","\n","# PREPARE MODEL, OPTIMIZER AND SCHEDULER\n","feature_extractor, model = create_model()\n","feature_extractor.eval()\n","model.eval()\n","\n","val_auc, val_accuracy, val_precision = evaluate_model(CFG, model, valid_loader, loss_criterion, epoch)\n","\n","# Log metrics to wandb\n","wandb.log({\n","    'val_auc': val_auc,\n","    'val_accuracy': val_accuracy,\n","    'val_precision': val_precision,\n","})\n","\n","features, targets = get_embeddings(feature_extractor, test_loader)\n","plot_tsne(features, targets)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4568125,"sourceId":7801430,"sourceType":"datasetVersion"},{"datasetId":4568781,"sourceId":7877494,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"08d2c86ce4ab4b9e813473170145d4af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96fb1b36ead648c4a4ebebb74c9fcf2c","placeholder":"​","style":"IPY_MODEL_af10206931434c6fbfde31af25affbfa","value":"model.safetensors: 100%"}},"103798aed0c64914b55a691a4d22253e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4420cb88a6a74540a671a529e7320589","placeholder":"​","style":"IPY_MODEL_6b7cb0c6580d4caba2c4f8385749a145","value":" 124M/124M [00:00&lt;00:00, 384MB/s]"}},"2831af4d288b45b688ba9b5992dce3f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3763df1b6564f319afc8d67073af72f","max":124450218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4c3f492a9b946a0b72df7ea6bb188a9","value":124450218}},"4420cb88a6a74540a671a529e7320589":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b7cb0c6580d4caba2c4f8385749a145":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87901707be784b03998e8b8093255ea6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96fb1b36ead648c4a4ebebb74c9fcf2c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3763df1b6564f319afc8d67073af72f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4c3f492a9b946a0b72df7ea6bb188a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af10206931434c6fbfde31af25affbfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b53d0ba4ae3e496092fdf021cb4097aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_08d2c86ce4ab4b9e813473170145d4af","IPY_MODEL_2831af4d288b45b688ba9b5992dce3f4","IPY_MODEL_103798aed0c64914b55a691a4d22253e"],"layout":"IPY_MODEL_87901707be784b03998e8b8093255ea6"}}}}},"nbformat":4,"nbformat_minor":4}
