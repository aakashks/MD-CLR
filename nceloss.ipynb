{"cells":[{"cell_type":"markdown","metadata":{"id":"JC-p4CpT1_Lc"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T05:12:51.017424Z","iopub.status.busy":"2024-04-16T05:12:51.016653Z","iopub.status.idle":"2024-04-16T05:12:51.028310Z","shell.execute_reply":"2024-04-16T05:12:51.027476Z","shell.execute_reply.started":"2024-04-16T05:12:51.017395Z"},"id":"gr9MjCphpOPx","trusted":true},"outputs":[],"source":["OUTPUT_FOLDER = \"/scratch/aakash_ks.iitr/dr-scnn/\"\n","DATA_FOLDER = \"/scratch/aakash_ks.iitr/data/diabetic-retinopathy/\"\n","# TRAIN_DATA_FOLDER = DATA_FOLDER + 'resized_train/'\n","TRAIN_DATA_FOLDER = DATA_FOLDER + 'resized_train_c/'\n","\n","# TEST_DATA_FOLDER = DATA_FOLDER + 'test/'"]},{"cell_type":"markdown","metadata":{"id":"dOaKi5h92DBb"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:13:45.703815Z","iopub.status.busy":"2024-04-14T16:13:45.703112Z","iopub.status.idle":"2024-04-14T16:13:47.998744Z","shell.execute_reply":"2024-04-14T16:13:47.997755Z","shell.execute_reply.started":"2024-04-14T16:13:45.703785Z"},"id":"NNdj2cxdkpiv","trusted":true},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","from PIL import Image\n","\n","plt.rcParams['figure.dpi'] = 200"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:13:48.000701Z","iopub.status.busy":"2024-04-14T16:13:48.000307Z","iopub.status.idle":"2024-04-14T16:13:55.723296Z","shell.execute_reply":"2024-04-14T16:13:55.722301Z","shell.execute_reply.started":"2024-04-14T16:13:48.000675Z"},"id":"7yoCqGCB2jIS","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n","from torchvision.transforms import v2\n","\n","import timm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T05:18:58.885632Z","iopub.status.busy":"2024-04-16T05:18:58.884804Z","iopub.status.idle":"2024-04-16T05:18:58.891553Z","shell.execute_reply":"2024-04-16T05:18:58.890585Z","shell.execute_reply.started":"2024-04-16T05:18:58.885598Z"},"id":"8Ejzj4rDx_GK","trusted":true},"outputs":[],"source":["NUM_CLASSES = 5\n","\n","class CFG:\n","    seed = 42\n","    N_folds = 5\n","    train_folds = [0, 1] # [0,1,2,3,4]\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    apex=True # use half precision\n","    workers = 16\n","\n","    model_name = \"resnet50.a1_in1k\"\n","    epochs = 20\n","    cropped = True\n","    # weights =  torch.tensor([0.206119, 0.793881],dtype=torch.float32)\n","\n","    clip_val = 1000.\n","    batch_size = 64\n","    # gradient_accumulation_steps = 1\n","\n","    lr = 1e-3\n","    weight_decay=1e-2\n","    \n","    resolution = 224\n","    samples_per_class = 500\n","    \n","    cl_margin = 2.0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T15:23:54.468659Z","iopub.status.busy":"2024-04-14T15:23:54.468265Z","iopub.status.idle":"2024-04-14T15:24:25.601196Z","shell.execute_reply":"2024-04-14T15:24:25.600218Z","shell.execute_reply.started":"2024-04-14T15:23:54.468630Z"},"trusted":true},"outputs":[],"source":["import wandb\n","# from kaggle_secrets import UserSecretsClient\n","# user_secrets = UserSecretsClient()\n","# wandb.login(key=user_secrets.get_secret(\"wandb_api\"))\n","\n","run = wandb.init(\n","    project=\"hello-world\", \n","    dir=OUTPUT_FOLDER,\n","    config={\n","    k:v for k, v in CFG.__dict__.items() if not k.startswith('__')}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(CFG.device)"]},{"cell_type":"markdown","metadata":{"id":"7Ve34id2b7uu"},"source":["# Load train data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:13:55.725554Z","iopub.status.busy":"2024-04-14T16:13:55.725188Z","iopub.status.idle":"2024-04-14T16:13:55.745976Z","shell.execute_reply":"2024-04-14T16:13:55.744974Z","shell.execute_reply.started":"2024-04-14T16:13:55.725520Z"},"id":"mq-oqFtvkpix","trusted":true},"outputs":[],"source":["# train_data = pd.read_csv(os.path.join(DATA_FOLDER, 'trainLabels.csv'))\n","train_data = pd.read_csv(os.path.join(DATA_FOLDER, 'trainLabels_cropped.csv')).sample(frac=1).reset_index(drop=True)\n","train_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# remove all images from the csv if they are not in the folder\n","lst = map(lambda x: x[:-5], os.listdir(TRAIN_DATA_FOLDER))\n","train_data = train_data[train_data.image.isin(lst)]\n","len(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.level.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# take only 100 samples from each class\n","train_data = train_data.groupby('level').head(CFG.samples_per_class).reset_index(drop=True)\n","train_data.level.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"1Mu24W3Xkpix"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.transforms import functional as func\n","\n","class CustomTransform:\n","    def __init__(self, output_size=(CFG.resolution, CFG.resolution), radius_factor=0.9):\n","        self.output_size = output_size\n","        self.radius_factor = radius_factor\n","\n","    def __call__(self, img):\n","        # Assuming img is a PIL Image\n","        # Normalize and preprocess as previously defined\n","        img = func.resize(img, int(min(img.size) / self.radius_factor))\n","        img_tensor = func.to_tensor(img)\n","        mean, std = img_tensor.mean([1, 2]), img_tensor.std([1, 2])\n","        img_normalized = func.normalize(img_tensor, mean.tolist(), std.tolist())\n","        kernel_size = 15\n","        padding = kernel_size // 2\n","        avg_pool = torch.nn.AvgPool2d(kernel_size, stride=1, padding=padding)\n","        local_avg = avg_pool(img_normalized.unsqueeze(0)).squeeze(0)\n","        img_subtracted = img_normalized - local_avg\n","        center_crop_size = int(min(img_subtracted.shape[1:]) * self.radius_factor)\n","        img_cropped = func.center_crop(img_subtracted, [center_crop_size, center_crop_size])\n","\n","        # Apply augmentations\n","        img_resized = func.resize(img_cropped, self.output_size)\n","\n","        return img_resized"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:14:03.100814Z","iopub.status.busy":"2024-04-14T16:14:03.100062Z","iopub.status.idle":"2024-04-14T16:14:03.108402Z","shell.execute_reply":"2024-04-14T16:14:03.107471Z","shell.execute_reply.started":"2024-04-14T16:14:03.100781Z"},"trusted":true},"outputs":[],"source":["# train_transforms = CustomTransform()\n","\n","train_transforms = v2.Compose([\n","    CustomTransform(),\n","    # v2.RandomResizedCrop(CFG.resolution, scale=(0.8, 1.0)),  # Krizhevsky style random cropping\n","    v2.RandomHorizontalFlip(),  # Random horizontal flip\n","    v2.RandomVerticalFlip(),  # Random vertical flip\n","    v2.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2)),  # Gaussian blur with random kernel size and sigma\n","    v2.RandomRotation(degrees=(0, 90)),  # Random rotation between 0 and 360 degrees\n","    v2.ToDtype(torch.float32, scale=False),\n","])\n","\n","val_transforms = v2.Compose([\n","    CustomTransform(),\n","    v2.ToDtype(torch.float32, scale=False),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:34.766966Z","iopub.status.busy":"2024-04-14T16:21:34.766584Z","iopub.status.idle":"2024-04-14T16:21:34.774717Z","shell.execute_reply":"2024-04-14T16:21:34.773808Z","shell.execute_reply.started":"2024-04-14T16:21:34.766935Z"},"id":"_mAcIdn2kpiy","scrolled":true,"trusted":true},"outputs":[],"source":["class ImageTrainDataset(Dataset):\n","    def __init__(\n","        self,\n","        folder,\n","        data,\n","        transforms,\n","    ):\n","        self.folder = folder\n","        self.data = data\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        d = self.data.loc[index]\n","        image = Image.open(f\"{self.folder}{d.image}.jpeg\")\n","        image = self.transforms(image)\n","        label = d.level\n","\n","        return image, torch.tensor(label, dtype=torch.long)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # visualize the transformations\n","# train_dataset = ImageTrainDataset(TRAIN_DATA_FOLDER, train_data, train_transforms)\n","# image, label = train_dataset[15]\n","# transformed_img_pil = func.to_pil_image(image)\n","# plt.imshow(transformed_img_pil)"]},{"cell_type":"markdown","metadata":{"id":"OzgB1JpAv3qg"},"source":["# Metric"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:35.200919Z","iopub.status.busy":"2024-04-14T16:21:35.200562Z","iopub.status.idle":"2024-04-14T16:21:35.205583Z","shell.execute_reply":"2024-04-14T16:21:35.204624Z","shell.execute_reply.started":"2024-04-14T16:21:35.200892Z"},"id":"WNxSAhBrxJ-G","trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score as sklearn_f1\n","from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, precision_score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:35.486917Z","iopub.status.busy":"2024-04-14T16:21:35.486197Z","iopub.status.idle":"2024-04-14T16:21:35.493226Z","shell.execute_reply":"2024-04-14T16:21:35.492187Z","shell.execute_reply.started":"2024-04-14T16:21:35.486887Z"},"id":"n0u9VgXTv7VU","trusted":true},"outputs":[],"source":["def nce_loss(embeddings, labels, temperature=0.1):\n","    embeddings = F.normalize(embeddings, p=2, dim=1)  # Normalize embeddings\n","    similarity_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n","    \n","    # Create targets for the positives: positions on the diagonal\n","    labels = labels.unsqueeze(0)\n","    positive_indices = torch.arange(embeddings.shape[0]).to(embeddings.device)\n","    positive_logits = similarity_matrix[positive_indices, positive_indices]\n","    \n","    # Mask out positive samples from log-sum-exp calculation\n","    mask = torch.eq(labels, labels.T).fill_diagonal_(0)\n","    negative_logits = similarity_matrix.masked_select(mask).view(embeddings.shape[0], -1)\n","    \n","    # Calculate log-sum-exp across negatives for each sample and subtract log of positive\n","    negative_logsumexp = torch.logsumexp(negative_logits, dim=1)\n","    loss = -torch.mean(positive_logits - negative_logsumexp)\n","    \n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"Zyfw9PLdkpiz"},"source":["# Train and evaluate functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class style:\n","    BLUE = '\\033[94m'\n","    GREEN = '\\033[92m'\n","    YELLOW = '\\033[93m'\n","    END = '\\033[0m'\n","    BOLD = '\\033[1m'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:36.151872Z","iopub.status.busy":"2024-04-14T16:21:36.151533Z","iopub.status.idle":"2024-04-14T16:21:36.156966Z","shell.execute_reply":"2024-04-14T16:21:36.155985Z","shell.execute_reply.started":"2024-04-14T16:21:36.151847Z"},"id":"KKt67LPn9YtB","trusted":true},"outputs":[],"source":["def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:36.353134Z","iopub.status.busy":"2024-04-14T16:21:36.352777Z","iopub.status.idle":"2024-04-14T16:21:36.363087Z","shell.execute_reply":"2024-04-14T16:21:36.362142Z","shell.execute_reply.started":"2024-04-14T16:21:36.353107Z"},"id":"yXcFJ6IYkpiz","trusted":true},"outputs":[],"source":["def evaluate_model(cfg, model, data_loader, loss_criterion, epoch=-1):\n","    # loss_fn = nn.CrossEntropyLoss(weight=cfg.weights.to(device), label_smoothing=0.1)\n","    loss_fn = loss_criterion\n","\n","    model.eval()\n","    val_loss = 0\n","\n","    total_len = len(data_loader)\n","    tk0 = tqdm(enumerate(data_loader), total=total_len)\n","    \n","    with torch.no_grad():\n","        for step, (images, labels) in tk0:\n","            images = images.to(device)\n","            target = labels.to(device)\n","\n","            ####################\n","            embeddings = model(images)\n","            loss = loss_fn(embeddings, target)\n","            ####################\n","            \n","            val_loss += loss.item()\n","\n","            del images, target\n","\n","    val_loss /= total_len\n","    # base_score, best_score, best_th = find_best_threshold(targets, predictions[:, 1])\n","    # For multi-class classification, you might need the class with the highest probability\n","\n","    print(f'Epoch {epoch}: validation loss = {val_loss:.4f}')\n","    return val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:36.532561Z","iopub.status.busy":"2024-04-14T16:21:36.532193Z","iopub.status.idle":"2024-04-14T16:21:36.545367Z","shell.execute_reply":"2024-04-14T16:21:36.544346Z","shell.execute_reply.started":"2024-04-14T16:21:36.532533Z"},"id":"nZFniP2hkpi0","trusted":true},"outputs":[],"source":["\n","def train_epoch(cfg, model, train_loader, loss_criterion, optimizer, scheduler, epoch):\n","    # scaler = torch.cuda.amp.GradScaler(enabled=cfg.apex)\n","    # loss_fn = nn.CrossEntropyLoss(weight=cfg.weights.to(device), label_smoothing=0.1)\n","    loss_fn = loss_criterion\n","\n","    model.train()\n","    train_loss = 0\n","    learning_rate_history = []\n","\n","    total_len = len(train_loader)\n","    tk0 = tqdm(enumerate(train_loader), total=total_len)\n","    for step, (images, labels) in tk0:\n","        images = images.to(device, non_blocking=True)\n","        target = labels.to(device, non_blocking=True)\n","\n","        ####################\n","        embeddings = model(images)\n","        loss = loss_fn(embeddings, target)\n","        ####################\n","\n","        # scaler.scale(loss).backward()\n","        loss.backward()\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg.clip_val)\n","\n","        train_loss += loss.item()\n","        # scaler.step(optimizer)\n","        # scaler.update()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        if scheduler is None:\n","            lr = optimizer.param_groups[0]['lr']\n","        else:\n","            scheduler.step()\n","            lr = scheduler.get_last_lr()[0]\n","\n","        tk0.set_description(f\"Epoch {epoch} training {step+1}/{total_len} [LR {lr:0.6f}] - loss: {train_loss/(step+1):.4f}\")\n","        learning_rate_history.append(lr)\n","\n","        del images, target\n","\n","    train_loss /= total_len\n","\n","    print(f'Epoch {epoch}: training loss = {train_loss:.4f}')\n","    return train_loss, learning_rate_history"]},{"cell_type":"markdown","metadata":{"id":"qN83vJk4xCA3"},"source":["# Train model"]},{"cell_type":"markdown","metadata":{"id":"8NyHYtzwZT8h"},"source":["## Split data\n","\n","The distribution of classes in the training data is not balance so using StratifiedKFold will ensure that the distrubution of positive and negative samples in all folds will match the original distributions."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:39.039930Z","iopub.status.busy":"2024-04-14T16:21:39.039564Z","iopub.status.idle":"2024-04-14T16:21:39.307011Z","shell.execute_reply":"2024-04-14T16:21:39.306082Z","shell.execute_reply.started":"2024-04-14T16:21:39.039904Z"},"id":"HaYXa749AEes","outputId":"65b6c941-0e6e-4503-b513-574264d657ce","trusted":true},"outputs":[],"source":["plt.figure(figsize=(4,2))\n","sns.histplot(train_data[\"level\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:39.309571Z","iopub.status.busy":"2024-04-14T16:21:39.308889Z","iopub.status.idle":"2024-04-14T16:21:39.321211Z","shell.execute_reply":"2024-04-14T16:21:39.320264Z","shell.execute_reply.started":"2024-04-14T16:21:39.309534Z"},"id":"DRHeo8pr56FX","trusted":true},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","\n","sgkf = StratifiedKFold(n_splits=CFG.N_folds, random_state=CFG.seed, shuffle=True)\n","for i, (train_index, test_index) in enumerate(sgkf.split(train_data[\"image\"].values, train_data[\"level\"].values)):\n","    train_data.loc[test_index, \"fold\"] = i"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def freeze_initial_layers(model, freeze_up_to_layer=3):\n","    # The ResNet50 features block is typically named 'layerX' in PyTorch\n","    layer_names = ['conv1', 'bn1', 'layer1', 'layer2', 'layer3', 'layer4']\n","    # Iterate over model children (first level only, adjust as needed)\n","    for name, child in model.named_children():\n","        if name in layer_names[:freeze_up_to_layer]:\n","            for param in child.parameters():\n","                param.requires_grad = False\n","            print(f'Layer {name} has been frozen.')\n","        else:\n","            print(f'Layer {name} is trainable.')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_model():\n","    model = timm.create_model(CFG.model_name, num_classes=NUM_CLASSES, pretrained=True)\n","\n","#     model = models.resnet50(models.ResNet50_Weights.SENTINEL2_ALL_DINO)\n","#     wd = torch.concat([model.conv1.weight[:, :13, ...], model.conv1.weight[:, :7, ...]], dim=1)\n","#     model.conv1 = nn.Conv2d(20, 64, 7, 2, 3, bias=False)\n","#     model.conv1.weight = nn.Parameter(wd)\n","#     model.fc = nn.Linear(in_features=2048, out_features=2, bias=True)\n","    freeze_initial_layers(model, freeze_up_to_layer=CFG.frozen_layers)\n","    return model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.manifold import TSNE\n","import matplotlib.colors as mcolors\n","\n","def get_embeddings(model, data_loader):\n","    model.eval()\n","    \n","    features = []\n","    targets = []\n","\n","    total_len = len(data_loader)\n","    tk0 = tqdm(enumerate(data_loader), total=total_len)\n","    with torch.no_grad():\n","        for step, (images, labels) in tk0:\n","            images = images.to(device)\n","            target = labels.to(device)\n","\n","            embds = model(images)\n","\n","            features.append(embds.detach().cpu())\n","            targets.append(target.detach().cpu())\n","\n","    features = torch.cat(features, dim=0)\n","    targets = torch.cat(targets, dim=0)\n","    \n","    # store the embeddings for future use\n","    torch.save(features, os.path.join(wandb.run.dir, f\"embeddings.pth\"))\n","    torch.save(targets, os.path.join(wandb.run.dir, f\"targets.pth\"))\n","\n","    return features, targets\n","\n","\n","def plot_tsne(embeddings, labels):\n","    # Apply t-SNE to the embeddings\n","    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n","    tsne_results = tsne.fit_transform(embeddings.numpy())\n","\n","    # Define the number of unique labels/classes\n","    num_classes = len(np.unique(labels.numpy()))\n","    # Create a custom color map with specific color transitions\n","    colors = ['blue', 'green', 'yellow', 'orange', 'red']\n","    cmap = mcolors.LinearSegmentedColormap.from_list(\"Custom\", colors, N=num_classes)\n","\n","    # Create a boundary norm with boundaries and colors\n","    norm = mcolors.BoundaryNorm(np.arange(-0.5, num_classes + 0.5, 1), cmap.N)\n","\n","    plt.figure(figsize=(10, 8))\n","    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap=cmap, norm=norm, alpha=0.7)\n","    colorbar = plt.colorbar(scatter, ticks=np.arange(num_classes))\n","    colorbar.set_label('Severity Level')\n","    colorbar.set_ticklabels(np.arange(num_classes))  # Set discrete labels if needed\n","    plt.title('t-SNE of Image Embeddings with Discrete Severity Levels')\n","    plt.xlabel('t-SNE Axis 1')\n","    plt.ylabel('t-SNE Axis 2')\n","    plt.show()\n","    plt.savefig(os.path.join(wandb.run.dir, f\"tsne.png\"), bbox_inches='tight')\n"]},{"cell_type":"markdown","metadata":{"id":"rF9BFqS8AXBY"},"source":["## Train folds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:40.983908Z","iopub.status.busy":"2024-04-14T16:21:40.983523Z","iopub.status.idle":"2024-04-14T16:21:48.524465Z","shell.execute_reply":"2024-04-14T16:21:48.523002Z","shell.execute_reply.started":"2024-04-14T16:21:40.983878Z"},"id":"7CFfmp3CxDG5","outputId":"952103d3-bbd9-449f-e9cf-26d5c2608aea","scrolled":true,"trusted":true},"outputs":[],"source":["for FOLD in CFG.train_folds:\n","    seed_everything(CFG.seed)\n","\n","    # PREPARE DATA\n","    fold_train_data = train_data[train_data[\"fold\"] != FOLD].reset_index(drop=True)\n","    fold_valid_data = train_data[train_data[\"fold\"] == FOLD].reset_index(drop=True)\n","\n","    train_dataset = ImageTrainDataset(TRAIN_DATA_FOLDER, fold_train_data, transforms=train_transforms)\n","    valid_dataset = ImageTrainDataset(TRAIN_DATA_FOLDER, fold_valid_data, transforms=val_transforms)\n","\n","    train_loader = DataLoader(\n","            train_dataset,\n","            batch_size=CFG.batch_size,\n","            shuffle=True,\n","            num_workers=CFG.workers,\n","            pin_memory=True,\n","            drop_last=True\n","        )\n","\n","    valid_loader = DataLoader(\n","        valid_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    # PREPARE MODEL, OPTIMIZER AND SCHEDULER\n","    model = create_model()\n","    print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):_}\")\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer, eta_min=1e-6, T_max =CFG.epochs * len(train_loader),\n","        )\n","    \n","    ######################\n","    loss_criterion = nce_loss\n","    ######################\n","\n","    # TRAIN FOLD\n","    best_loss = float('inf')\n","    no_improve_epochs = 0\n","    patience = 5  # Number of epochs to wait before stopping if no improvement\n","\n","    \n","    wandb.run.tags = [f\"fold_{FOLD}\"]\n","    \n","    for epoch in range(0, CFG.epochs):\n","        train_loss, train_lr = train_epoch(CFG, model, train_loader, loss_criterion, optimizer, scheduler, epoch)\n","\n","        val_loss = evaluate_model(CFG, model, valid_loader, loss_criterion, epoch)\n","        \n","        # Log metrics to wandb\n","        wandb.log({\n","            'train_loss': train_loss,\n","            'val_loss': val_loss,\n","            'learning_rate': train_lr[-1]  # Log the last learning rate of the epoch\n","        })\n","\n","\n","    if (val_loss < best_loss):\n","        print(f\"{style.GREEN}New best loss: {best_loss:.4f} -> {val_loss:.4f}{style.END}\")\n","        best_loss = val_loss\n","        no_improve_epochs = 0  # Reset the counter since we improved\n","        torch.save(model.state_dict(), os.path.join(wandb.run.dir, f'best_model_fold_{FOLD}.pth'))\n","    else:\n","        no_improve_epochs += 1\n","        print(f\"No improvement for {no_improve_epochs} epochs\")\n","\n","    if no_improve_epochs >= patience:\n","        print(f\"Stopping early after {no_improve_epochs} epochs without improvement\")\n","        break\n","    \n","    \n","    torch.save(model.state_dict(), os.path.join(wandb.run.dir, f'best_model_fold_{FOLD}.pth'))\n","            \n","\n","    # plot a tsne plot of all the images using embeddings from the model\n","    full_dataset = ImageTrainDataset(TRAIN_DATA_FOLDER, train_data, transforms=val_transforms)\n","    loader = DataLoader(\n","        full_dataset,\n","        batch_size=CFG.batch_size,\n","        shuffle=False,\n","        num_workers=CFG.workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","    \n","    features, targets = get_embeddings(model, loader)\n","    plot_tsne(features, targets)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4568125,"sourceId":7801430,"sourceType":"datasetVersion"},{"datasetId":4568781,"sourceId":7877494,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"08d2c86ce4ab4b9e813473170145d4af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96fb1b36ead648c4a4ebebb74c9fcf2c","placeholder":"​","style":"IPY_MODEL_af10206931434c6fbfde31af25affbfa","value":"model.safetensors: 100%"}},"103798aed0c64914b55a691a4d22253e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4420cb88a6a74540a671a529e7320589","placeholder":"​","style":"IPY_MODEL_6b7cb0c6580d4caba2c4f8385749a145","value":" 124M/124M [00:00&lt;00:00, 384MB/s]"}},"2831af4d288b45b688ba9b5992dce3f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3763df1b6564f319afc8d67073af72f","max":124450218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4c3f492a9b946a0b72df7ea6bb188a9","value":124450218}},"4420cb88a6a74540a671a529e7320589":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b7cb0c6580d4caba2c4f8385749a145":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87901707be784b03998e8b8093255ea6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96fb1b36ead648c4a4ebebb74c9fcf2c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3763df1b6564f319afc8d67073af72f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4c3f492a9b946a0b72df7ea6bb188a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af10206931434c6fbfde31af25affbfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b53d0ba4ae3e496092fdf021cb4097aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_08d2c86ce4ab4b9e813473170145d4af","IPY_MODEL_2831af4d288b45b688ba9b5992dce3f4","IPY_MODEL_103798aed0c64914b55a691a4d22253e"],"layout":"IPY_MODEL_87901707be784b03998e8b8093255ea6"}}}}},"nbformat":4,"nbformat_minor":4}
