{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = \"/scratch/aakash_ks.iitr/dr-scnn/\"\n",
    "DATA_FOLDER = \"/scratch/aakash_ks.iitr/data/diabetic-retinopathy/\"\n",
    "TRAIN_DATA_FOLDER = DATA_FOLDER + 'resized_train_c/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch modules\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils import data \n",
    "import torch.nn.functional as F \n",
    "from torchvision import transforms, models\n",
    "\n",
    "# other modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import timm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5\n",
    "\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    N_folds = 5\n",
    "    train_folds = [0] # [0,1,2,3,4]\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    apex=True # use half precision\n",
    "    workers = 16\n",
    "\n",
    "    model_name = \"resnet50.a1_in1k\"\n",
    "    epochs = 50\n",
    "    cropped = True\n",
    "    # weights =  torch.tensor([0.206119, 0.793881],dtype=torch.float32)\n",
    "\n",
    "    clip_val = 1000.\n",
    "    batch_size = 64\n",
    "    # gradient_accumulation_steps = 1\n",
    "\n",
    "    lr = 6e-3\n",
    "    weight_decay=1e-2\n",
    "    \n",
    "    resolution = 224\n",
    "    samples_per_class = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv(os.path.join(DATA_FOLDER, 'trainLabels.csv'))\n",
    "train_data = pd.read_csv(os.path.join(DATA_FOLDER, 'trainLabels_cropped.csv'))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all images from the csv if they are not in the folder\n",
    "lst = map(lambda x: x[:-5], os.listdir(TRAIN_DATA_FOLDER))\n",
    "train_data = train_data[train_data.image.isin(lst)].reset_index(drop=True)\n",
    "train_data = train_data.groupby('level').head(CFG.samples_per_class).reset_index(drop=True)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as func\n",
    "\n",
    "class CustomTransform:\n",
    "    def __init__(self, output_size=(CFG.resolution, CFG.resolution), radius_factor=0.9):\n",
    "        self.output_size = output_size\n",
    "        self.radius_factor = radius_factor\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Assuming img is a PIL Image\n",
    "        # Normalize and preprocess as previously defined\n",
    "        img = func.resize(img, int(min(img.size) / self.radius_factor))\n",
    "        img_tensor = func.to_tensor(img)\n",
    "        mean, std = img_tensor.mean([1, 2]), img_tensor.std([1, 2])\n",
    "        img_normalized = func.normalize(img_tensor, mean.tolist(), std.tolist())\n",
    "        kernel_size = 15\n",
    "        padding = kernel_size // 2\n",
    "        avg_pool = torch.nn.AvgPool2d(kernel_size, stride=1, padding=padding)\n",
    "        local_avg = avg_pool(img_normalized.unsqueeze(0)).squeeze(0)\n",
    "        img_subtracted = img_normalized - local_avg\n",
    "        center_crop_size = int(min(img_subtracted.shape[1:]) * self.radius_factor)\n",
    "        img_cropped = func.center_crop(img_subtracted, [center_crop_size, center_crop_size])\n",
    "\n",
    "        # Apply augmentations\n",
    "        img_resized = func.resize(img_cropped, self.output_size)\n",
    "\n",
    "        return img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    CustomTransform(),\n",
    "    v2.ToDtype(torch.float32, scale=False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTrainDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder,\n",
    "        data,\n",
    "        transforms,\n",
    "    ):\n",
    "        self.folder = folder\n",
    "        self.data = data\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data.loc[index]\n",
    "        image = Image.open(f\"{self.folder}{d.image}.jpeg\")\n",
    "        image = self.transforms(image)\n",
    "        label = d.level\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the transformations\n",
    "train_dataset = ImageTrainDataset(TRAIN_DATA_FOLDER, train_data, transform)\n",
    "image, label = train_dataset[15]\n",
    "transformed_img_pil = func.to_pil_image(image)\n",
    "plt.imshow(transformed_img_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce ROC and PR curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wandb.login(key=user_secrets.get_secret(\"wandb_api\"))\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"aml\", \n",
    "    dir=OUTPUT_FOLDER,\n",
    "    config={\n",
    "    k:v for k, v in CFG.__dict__.items() if not k.startswith('__')}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = timm.create_model(CFG.model_name, num_classes=5, checkpoint_path=OUTPUT_FOLDER + 'ckpt_epoch_8.pth')\n",
    "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=CFG.workers)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to train and then evaluate a linear classifier to evaluate these embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def evaluate_model(cfg, model, data_loader, epoch=-1):\n",
    "    model.eval()\n",
    "\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    total_len = len(data_loader)\n",
    "    tk0 = tqdm(enumerate(data_loader), total=total_len)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (images, labels) in tk0:\n",
    "            images = images.to(device)\n",
    "            target = labels.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "\n",
    "            targets.append(target.detach().cpu())\n",
    "            predictions.append(logits.detach().cpu())\n",
    "            del images, target, logits\n",
    "\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    probabilities = F.softmax(predictions, dim=1)\n",
    "\n",
    "    val_loss /= total_len\n",
    "    # base_score, best_score, best_th = find_best_threshold(targets, predictions[:, 1])\n",
    "    # For multi-class classification, you might need the class with the highest probability\n",
    "    predicted_classes = predictions.argmax(dim=1)\n",
    "\n",
    "    try:\n",
    "        wandb.log({\"roc\": wandb.plot.roc_curve(targets.numpy(), probabilities.numpy())})\n",
    "        roc_auc = roc_auc_score(targets.numpy(), probabilities.numpy(), multi_class='ovo')\n",
    "        \n",
    "        # wandb.log({\"pr\": wandb.plot.pr_curve(targets.numpy(), probabilities.numpy())})\n",
    "        \n",
    "        # cm = wandb.plot.confusion_matrix(\n",
    "        #     y_true=targets.numpy(), preds=probabilities.numpy(), class_names=['0', '1', '2', '3', '4']\n",
    "        # )\n",
    "\n",
    "        # wandb.log({\"confusion_matrix\": cm})\n",
    "    except:\n",
    "        roc_auc = 0\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(targets.numpy(), predicted_classes.numpy())\n",
    "\n",
    "    precision = precision_score(targets.numpy(), predicted_classes.numpy(), average='weighted')\n",
    "\n",
    "    print(f'Epoch {epoch}: validation loss = {val_loss:.4f} auc = {roc_auc:.4f} accuracy = {accuracy:.4f} precision = {precision:.4f}')\n",
    "    return val_loss, roc_auc, accuracy, precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
