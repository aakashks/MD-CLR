{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#/r"]},{"cell_type":"markdown","metadata":{"id":"JC-p4CpT1_Lc"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T05:12:51.017424Z","iopub.status.busy":"2024-04-16T05:12:51.016653Z","iopub.status.idle":"2024-04-16T05:12:51.028310Z","shell.execute_reply":"2024-04-16T05:12:51.027476Z","shell.execute_reply.started":"2024-04-16T05:12:51.017395Z"},"id":"gr9MjCphpOPx","trusted":true},"outputs":[],"source":["OUTPUT_FOLDER = \"/scratch/aakash_ks.iitr/dr-scnn/\"\n","DATA_FOLDER = \"/scratch/aakash_ks.iitr/data/diabetic-retinopathy/\"\n","# TRAIN_DATA_FOLDER = DATA_FOLDER + 'resized_train/'\n","TRAIN_DATA_FOLDER = DATA_FOLDER + 'resized_train_c/'\n","\n","TEST_DATA_FOLDER = DATA_FOLDER + 'test/'"]},{"cell_type":"markdown","metadata":{"id":"dOaKi5h92DBb"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:13:45.703815Z","iopub.status.busy":"2024-04-14T16:13:45.703112Z","iopub.status.idle":"2024-04-14T16:13:47.998744Z","shell.execute_reply":"2024-04-14T16:13:47.997755Z","shell.execute_reply.started":"2024-04-14T16:13:45.703785Z"},"id":"NNdj2cxdkpiv","trusted":true},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","from PIL import Image\n","\n","plt.rcParams['figure.dpi'] = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:13:48.000701Z","iopub.status.busy":"2024-04-14T16:13:48.000307Z","iopub.status.idle":"2024-04-14T16:13:55.723296Z","shell.execute_reply":"2024-04-14T16:13:55.722301Z","shell.execute_reply.started":"2024-04-14T16:13:48.000675Z"},"id":"7yoCqGCB2jIS","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n","from torchvision.transforms import v2\n","\n","import timm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T05:18:58.885632Z","iopub.status.busy":"2024-04-16T05:18:58.884804Z","iopub.status.idle":"2024-04-16T05:18:58.891553Z","shell.execute_reply":"2024-04-16T05:18:58.890585Z","shell.execute_reply.started":"2024-04-16T05:18:58.885598Z"},"id":"8Ejzj4rDx_GK","trusted":true},"outputs":[],"source":["\n","NUM_CLASSES = 5\n","\n","class CFG:\n","    seed = 42\n","    N_folds = 6\n","    train_folds = [0, ] # [0,1,2,3,4]\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    apex=True # use half precision\n","    workers = 16\n","\n","    model_name = \"resnet50.a1_in1k\"\n","    epochs = 20\n","    cropped = True\n","    # weights =  torch.tensor([0.206119, 0.793881],dtype=torch.float32)\n","\n","    clip_val = 1000.\n","    batch_size = 64\n","    # gradient_accumulation_steps = 1\n","\n","    lr = 5e-3\n","    weight_decay=1e-2\n","    \n","    resolution = 224\n","    samples_per_class = 1000\n","    frozen_layers = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T15:23:54.468659Z","iopub.status.busy":"2024-04-14T15:23:54.468265Z","iopub.status.idle":"2024-04-14T15:24:25.601196Z","shell.execute_reply":"2024-04-14T15:24:25.600218Z","shell.execute_reply.started":"2024-04-14T15:23:54.468630Z"},"trusted":true},"outputs":[],"source":["import wandb\n","# from kaggle_secrets import UserSecretsClient\n","# user_secrets = UserSecretsClient()\n","# wandb.login(key=user_secrets.get_secret(\"wandb_api\"))\n","\n","run = wandb.init(\n","    project=\"hello-world\", \n","    dir=OUTPUT_FOLDER,\n","    config={\n","    k:v for k, v in CFG.__dict__.items() if not k.startswith('__')}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.manifold import TSNE\n","import matplotlib.colors as mcolors\n","\n","class LinearClassifier(nn.Module):\n","    def __init__(self, in_features=2048, num_classes=NUM_CLASSES):\n","        super().__init__()\n","        self.model = nn.Linear(in_features, num_classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","    \n","\n","class SupConModel(nn.Module):\n","    def __init__(self, encoder, input_dim=2048, output_dim=128):        # assuming either resnet50 or resnet101 is used\n","        super().__init__()\n","        self.encoder = encoder\n","        self.head = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, output_dim)\n","        )\n","    \n","    def forward(self, x):\n","        ft = self.encoder(x)\n","        return F.normalize(self.head(ft), dim=1)\n","\n","\n","class ImageTrainDataset(Dataset):\n","    def __init__(\n","            self,\n","            folder,\n","            data,\n","            transforms,\n","    ):\n","        self.folder = folder\n","        self.data = data\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        d = self.data.loc[index]\n","        image = Image.open(f\"{self.folder}{d.image}.jpeg\")\n","        image = self.transforms(image)\n","        label = d.level\n","\n","        return image, torch.tensor(label, dtype=torch.long)\n","\n","\n","def plot_tsne(embeddings, labels):\n","    # Apply t-SNE to the embeddings\n","    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n","    tsne_results = tsne.fit_transform(embeddings.numpy())\n","\n","    # Define the number of unique labels/classes\n","    num_classes = len(np.unique(labels.numpy()))\n","    # Create a custom color map with specific color transitions\n","    colors = ['blue', 'green', 'yellow', 'orange', 'red']\n","    cmap = mcolors.LinearSegmentedColormap.from_list(\"Custom\", colors, N=num_classes)\n","\n","    # Create a boundary norm with boundaries and colors\n","    norm = mcolors.BoundaryNorm(np.arange(-0.5, num_classes + 0.5, 1), cmap.N)\n","\n","    fig = plt.figure(figsize=(10, 8))\n","    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap=cmap, norm=norm, alpha=0.5)\n","    colorbar = plt.colorbar(scatter, ticks=np.arange(num_classes))\n","    colorbar.set_label('Severity Level')\n","    colorbar.set_ticklabels(np.arange(num_classes))  # Set discrete labels if needed\n","    plt.title('t-SNE of Image Embeddings with Discrete Severity Levels')\n","    plt.xlabel('t-SNE Axis 1')\n","    plt.ylabel('t-SNE Axis 2')\n","    fg = wandb.Image(fig)\n","    wandb.log({\"t-SNE\": fg})\n","    plt.savefig(os.path.join(wandb.run.dir, f\"tsne.png\"), dpi=300, bbox_inches='tight')\n","\n","\n","\n","class style:\n","    BLUE = '\\033[94m'\n","    GREEN = '\\033[92m'\n","    YELLOW = '\\033[93m'\n","    END = '\\033[0m'\n","    BOLD = '\\033[1m'\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(CFG.device)"]},{"cell_type":"markdown","metadata":{"id":"1Mu24W3Xkpix"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.transforms import functional as func\n","\n","class CustomTransform:\n","    def __init__(self, output_size=(CFG.resolution, CFG.resolution), radius_factor=0.9):\n","        self.output_size = output_size\n","        self.radius_factor = radius_factor\n","\n","    def __call__(self, img):\n","        # Assuming img is a PIL Image\n","        # Normalize and preprocess as previously defined\n","        img = func.resize(img, int(min(img.size) / self.radius_factor))\n","        img_tensor = func.to_tensor(img)\n","        mean, std = img_tensor.mean([1, 2]), img_tensor.std([1, 2])\n","        img_normalized = func.normalize(img_tensor, mean.tolist(), std.tolist())\n","        kernel_size = 15\n","        padding = kernel_size // 2\n","        avg_pool = torch.nn.AvgPool2d(kernel_size, stride=1, padding=padding)\n","        local_avg = avg_pool(img_normalized.unsqueeze(0)).squeeze(0)\n","        img_subtracted = img_normalized - local_avg\n","        center_crop_size = int(min(img_subtracted.shape[1:]) * self.radius_factor)\n","        img_cropped = func.center_crop(img_subtracted, [center_crop_size, center_crop_size])\n","\n","        # Apply augmentations\n","        img_resized = func.resize(img_cropped, self.output_size)\n","\n","        return img_resized"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:14:03.100814Z","iopub.status.busy":"2024-04-14T16:14:03.100062Z","iopub.status.idle":"2024-04-14T16:14:03.108402Z","shell.execute_reply":"2024-04-14T16:14:03.107471Z","shell.execute_reply.started":"2024-04-14T16:14:03.100781Z"},"trusted":true},"outputs":[],"source":["# train_transforms = CustomTransform()\n","\n","train_transforms = v2.Compose([\n","    v2.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2)),  # Gaussian blur with random kernel size and sigma\n","    v2.RandomRotation(degrees=(0, 90)),  # Random rotation between 0 and 360 degrees\n","    CustomTransform(),\n","    # v2.RandomResizedCrop(CFG.resolution, scale=(0.8, 1.0)),  # Krizhevsky style random cropping\n","    v2.RandomHorizontalFlip(),  # Random horizontal flip\n","    v2.RandomVerticalFlip(),  # Random vertical flip\n","    v2.ToDtype(torch.float32, scale=False),\n","])\n","\n","val_transforms = v2.Compose([\n","    CustomTransform(),\n","    v2.ToDtype(torch.float32, scale=False),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.listdir(TEST_DATA_FOLDER )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from torchvision.datasets import ImageFolder\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import random_split\n","\n","# visualize the transformations\n","data = ImageFolder(TEST_DATA_FOLDER, transform=val_transforms)\n","\n","# Define the ratio for the split\n","train_ratio = 0.75  # 75% training, 25% validation\n","train_size = int(train_ratio * len(data))\n","val_size = len(data) - train_size\n","\n","# Split the dataset\n","train_dataset, val_dataset = random_split(data, [train_size, val_size])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(val_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# split the data into train and validation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"OzgB1JpAv3qg"},"source":["# Metric"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:35.200919Z","iopub.status.busy":"2024-04-14T16:21:35.200562Z","iopub.status.idle":"2024-04-14T16:21:35.205583Z","shell.execute_reply":"2024-04-14T16:21:35.204624Z","shell.execute_reply.started":"2024-04-14T16:21:35.200892Z"},"id":"WNxSAhBrxJ-G","trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score as sklearn_f1\n","from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, precision_score"]},{"cell_type":"markdown","metadata":{"id":"Zyfw9PLdkpiz"},"source":["# Train and evaluate functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:36.353134Z","iopub.status.busy":"2024-04-14T16:21:36.352777Z","iopub.status.idle":"2024-04-14T16:21:36.363087Z","shell.execute_reply":"2024-04-14T16:21:36.362142Z","shell.execute_reply.started":"2024-04-14T16:21:36.353107Z"},"id":"yXcFJ6IYkpiz","trusted":true},"outputs":[],"source":["def evaluate_model(cfg, feature_extractor, model, data_loader, epoch=-1):\n","    targets = []\n","    predictions = []\n","\n","    total_len = len(data_loader)\n","    tk0 = tqdm(enumerate(data_loader), total=total_len)\n","    \n","    with torch.no_grad():\n","        for step, (images, labels) in tk0:\n","            images = images.to(device)\n","            target = labels.to(device)\n","            \n","            features = feature_extractor(images)\n","            logits = model(features)\n","            \n","            targets.append(target.detach().cpu())\n","            predictions.append(logits.detach().cpu())\n","            del images, target, logits\n","\n","    targets = torch.cat(targets, dim=0)\n","    predictions = torch.cat(predictions, dim=0)\n","    probabilities = F.softmax(predictions, dim=1)\n","\n","    # base_score, best_score, best_th = find_best_threshold(targets, predictions[:, 1])\n","    # For multi-class classification, you might need the class with the highest probability\n","    predicted_classes = predictions.argmax(dim=1)\n","\n","    try:\n","        wandb.log({\"roc\": wandb.plot.roc_curve(targets.numpy(), probabilities.numpy())})\n","        roc_auc = roc_auc_score(targets.numpy(), probabilities.numpy(), multi_class='ovo')\n","        \n","        wandb.log({\"pr\": wandb.plot.pr_curve(targets.numpy(), probabilities.numpy())})\n","        \n","    except:\n","        roc_auc = 0\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(targets.numpy(), predicted_classes.numpy())\n","\n","    precision = precision_score(targets.numpy(), predicted_classes.numpy(), average='weighted')\n","\n","    print(f'Epoch {epoch}: auc = {roc_auc:.4f} accuracy = {accuracy:.4f} precision = {precision:.4f}')\n","    return roc_auc, accuracy, precision"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:40.091094Z","iopub.status.busy":"2024-04-14T16:21:40.090402Z","iopub.status.idle":"2024-04-14T16:21:40.096027Z","shell.execute_reply":"2024-04-14T16:21:40.095120Z","shell.execute_reply.started":"2024-04-14T16:21:40.091064Z"},"trusted":true},"outputs":[],"source":["def create_model():\n","    # get the feature extractor\n","    resnet = timm.create_model(CFG.model_name, num_classes=0, pretrained=False)\n","    feature_extractor = SupConModel(resnet)\n","    feature_extractor.load_state_dict(torch.load(OUTPUT_FOLDER + 'ckpt_epoch_8.pth'))\n","    \n","    # remove the projection head\n","    feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])\n","\n","    # create a simple linear classifier\n","    classifier = LinearClassifier()\n","    classifier.load_state_dict(torch.load(OUTPUT_FOLDER + 'lc_sclr_11.pth'))\n","    return feature_extractor.to(device), classifier.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.manifold import TSNE\n","import matplotlib.colors as mcolors\n","\n","def get_embeddings(model, data_loader):\n","    model.eval()\n","    \n","    features = []\n","    targets = []\n","\n","    total_len = len(data_loader)\n","    tk0 = tqdm(enumerate(data_loader), total=total_len)\n","    with torch.no_grad():\n","        for step, (images, labels) in tk0:\n","            images = images.to(device)\n","            target = labels.to(device)\n","\n","            embds = model(images)\n","\n","            features.append(embds.detach().cpu())\n","            targets.append(target.detach().cpu())\n","\n","    features = torch.cat(features, dim=0)\n","    targets = torch.cat(targets, dim=0)\n","    \n","    # # store the embeddings for future use\n","    # torch.save(features, os.path.join(wandb.run.dir, f\"embeddings.pth\"))\n","    # torch.save(targets, os.path.join(wandb.run.dir, f\"targets.pth\"))\n","\n","    return features, targets\n"]},{"cell_type":"markdown","metadata":{"id":"rF9BFqS8AXBY"},"source":["## Train folds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:21:40.983908Z","iopub.status.busy":"2024-04-14T16:21:40.983523Z","iopub.status.idle":"2024-04-14T16:21:48.524465Z","shell.execute_reply":"2024-04-14T16:21:48.523002Z","shell.execute_reply.started":"2024-04-14T16:21:40.983878Z"},"id":"7CFfmp3CxDG5","outputId":"952103d3-bbd9-449f-e9cf-26d5c2608aea","scrolled":true,"trusted":true},"outputs":[],"source":["seed_everything(CFG.seed)\n","\n","test_loader = DataLoader(\n","    test_data,\n","    batch_size=CFG.batch_size,\n","    shuffle=False,\n","    num_workers=CFG.workers,\n","    pin_memory=True,\n","    drop_last=False,\n",")\n","\n","# PREPARE MODEL, OPTIMIZER AND SCHEDULER\n","feature_extractor, model = create_model()\n","feature_extractor.eval()\n","model.eval()\n","\n","val_auc, val_accuracy, val_precision = evaluate_model(CFG, model, valid_loader, loss_criterion, epoch)\n","\n","# Log metrics to wandb\n","wandb.log({\n","    'val_auc': val_auc,\n","    'val_accuracy': val_accuracy,\n","    'val_precision': val_precision,\n","})\n","\n","features, targets = get_embeddings(feature_extractor, test_loader)\n","plot_tsne(features, targets)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4568125,"sourceId":7801430,"sourceType":"datasetVersion"},{"datasetId":4568781,"sourceId":7877494,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"08d2c86ce4ab4b9e813473170145d4af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96fb1b36ead648c4a4ebebb74c9fcf2c","placeholder":"​","style":"IPY_MODEL_af10206931434c6fbfde31af25affbfa","value":"model.safetensors: 100%"}},"103798aed0c64914b55a691a4d22253e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4420cb88a6a74540a671a529e7320589","placeholder":"​","style":"IPY_MODEL_6b7cb0c6580d4caba2c4f8385749a145","value":" 124M/124M [00:00&lt;00:00, 384MB/s]"}},"2831af4d288b45b688ba9b5992dce3f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3763df1b6564f319afc8d67073af72f","max":124450218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4c3f492a9b946a0b72df7ea6bb188a9","value":124450218}},"4420cb88a6a74540a671a529e7320589":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b7cb0c6580d4caba2c4f8385749a145":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87901707be784b03998e8b8093255ea6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96fb1b36ead648c4a4ebebb74c9fcf2c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3763df1b6564f319afc8d67073af72f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4c3f492a9b946a0b72df7ea6bb188a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af10206931434c6fbfde31af25affbfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b53d0ba4ae3e496092fdf021cb4097aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_08d2c86ce4ab4b9e813473170145d4af","IPY_MODEL_2831af4d288b45b688ba9b5992dce3f4","IPY_MODEL_103798aed0c64914b55a691a4d22253e"],"layout":"IPY_MODEL_87901707be784b03998e8b8093255ea6"}}}}},"nbformat":4,"nbformat_minor":4}
